# -*- coding: utf-8 -*-
"""BiGRU_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DndLGVwlDqwYvjz1UcA5FscKkc9Z0Vsv
"""

from google.colab import drive
drive.mount('/content/drive' )

import numpy as np 
import pandas as pd
import tensorflow as tf 
import random
import pickle 
import gc
import os
from sklearn import metrics
from sklearn.model_selection import train_test_split
import sys
import matplotlib.pyplot as plt
from matplotlib import gridspec
sys.path.append('/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification/tools')
sys.path.append('/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification')
from tools_benchmark import  compute_bias_metrics_for_model, calculate_overall_auc,get_final_metric
from tools_load_data import get_datasets
from tools_evaluate_model import evaluate, plot_history_for_accuracy_and_loss

"""# TPU Configs"""

# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.get_strategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)

MODEL_NO = 2 
BATCH_SIZE = 512 
EPOCHS = 10

"""# Load Datasets"""

IDENTITY_COLUMNS  = [
    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',
    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'
  ]

train_df = pd.read_csv("/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification/data/train_cleared.csv")
train_df = train_df[:10000]
test_public_df = pd.read_csv("/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification/data/test_public_cleared.csv")
test_public_df = test_public_df.loc[:, ['toxicity','comment_text']  + IDENTITY_COLUMNS ].dropna()[:1000]
test_private_df = pd.read_csv("/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification/data/test_private_cleared.csv")
test_private_df = test_private_df.loc[:, ['toxicity', 'comment_text'] + IDENTITY_COLUMNS ].dropna()[:1000]

# split 
train_df , val_df = train_test_split(train_df,test_size = 0.2 , random_state = 13 , shuffle = True)

train_dataset,validation_dataset, public_test_dataset, private_test_dataset , embedding_matrix = get_datasets(train_df, val_df, test_public_df, test_private_df)

"""# Training and evaluation of models """

class CustomStopper(tf.keras.callbacks.EarlyStopping):
    def __init__(self, monitor='val_loss',
             min_delta=0, patience=0, verbose=0, mode='auto', start_epoch = 2 , restore_best_weights = True): # add argument for starting epoch
        super(CustomStopper, self).__init__(monitor=monitor,patience=patience,min_delta=min_delta,mode=mode, restore_best_weights = restore_best_weights)
        self.start_epoch = start_epoch

    def on_epoch_end(self, epoch, logs=None):
        if epoch > self.start_epoch:
            super().on_epoch_end(epoch, logs)

es = CustomStopper(start_epoch=2)

"""## Build model"""

def create_BiGRU(embedding_matrix, num_of_hidden_layers , gru_units, dense_units):
  words = tf.keras.Input(shape=(None,))
  x = tf.keras.layers.Embedding(input_dim=embedding_matrix.shape[0],
                                 output_dim = embedding_matrix.shape[1],
                                 weights=[embedding_matrix], trainable=False)(words)
  x = tf.keras.layers.SpatialDropout1D(0.2)(x)
  if num_of_hidden_layers == 2 : 
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units,return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units,return_sequences=True))(x)
  elif num_of_hidden_layers == 3 : 
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units,return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units,return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units,return_sequences=True))(x)
  else:
    print("Wrong number of hidden layers")
    return
  
  
  
  avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)
  max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)

  GRU_out = tf.keras.layers.concatenate([max_pool, avg_pool])

  # skip connections 
  x = tf.keras.layers.add([GRU_out,tf.keras.layers.Dense(dense_units,activation='relu')(GRU_out)]) 
  x = tf.keras.layers.add([x,tf.keras.layers.Dense(dense_units,activation='relu')(x)]) 
  
  result = tf.keras.layers.Dense(1, activation='sigmoid', name = 'target' )(x)
  aux_result =  tf.keras.layers.Dense(6, activation='sigmoid' , name = 'aux')(x)
    
  model = tf.keras.Model(inputs=words, outputs=[result, aux_result])
  model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

  return model

"""## Train and tune the model"""

def train(number_of_hidden_GRU_layers, gru_units, dense_units):
  histories = [] 
  public_predictions = [] 
  private_predictions = [] 
  MODEL_NAME = "BiGRU" + str(number_of_hidden_GRU_layers)+"-"+str(gru_units)
  print("\nBiGRU" + str(number_of_hidden_GRU_layers)+"-"+str(gru_units))
  PATH = 'BiGRU/' + MODEL_NAME
      
  for i in range(MODEL_NO):
    
    with strategy.scope():
      # create model 
      BiGRU = create_BiGRU(embedding_matrix,number_of_hidden_GRU_layers, gru_units, dense_units)
    
    BiGRU.summary()
    n_steps = len(train_df) // BATCH_SIZE 
    history = BiGRU.fit(x = train_dataset,validation_data=validation_dataset , callbacks=[es] , epochs = EPOCHS 
                          ,verbose = 1 , steps_per_epoch=n_steps)   
    histories.append(history)

    public_predictions.append(BiGRU.predict(public_test_dataset, verbose=1 )[0].flatten())
    private_predictions.append(BiGRU.predict(private_test_dataset, verbose=1 )[0].flatten())
    #save memmory 
    tf.tpu.experimental.initialize_tpu_system(tpu)

  y_public_pred = np.average(public_predictions, axis =0 )
  y_private_pred = np.average(private_predictions, axis= 0)
  # save its graph 
  tf.keras.utils.plot_model(BiGRU, show_shapes= True ,show_layer_names=False, 
                  to_file='/content/drive/My Drive/Jigsaw Unintended Bias in Toxicity Classification/models/BiGRU/' + 
                  MODEL_NAME +'/BiGRU.png')
  
  evaluate(y_public_pred,y_private_pred, test_public_df, test_private_df, PATH , MODEL_NAME)
  plot_history_for_accuracy_and_loss(histories, PATH)
  
  # delete to save memmory 
  del BiGRU
  del y_public_pred
  del y_private_pred
  del histories 
  gc.collect()

"""### 2-LAYERS

#### BiGRU2-64
"""

number_of_hidden_GRU_layers = 2 
gru_units = 64
dense_units = 4 * gru_units # * 4 because of concanation and bidirecrionality  
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""#### BiGRU2-128"""

number_of_hidden_GRU_layers = 2 
gru_units = 128
dense_units = 4 * gru_units # * 4 because of concanation and bidirecrionality  
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""#### BiGRU2-256"""

number_of_hidden_GRU_layers = 2 
gru_units = 256
dense_units = 4 * gru_units # * 4 because of concanation and bidirecrionality  
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""### 3-LAYERS

#### BiGRU3-64
"""

number_of_hidden_GRU_layers = 3 
gru_units = 64
dense_units = 4 * gru_units
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""#### BiGRU3-128"""

number_of_hidden_GRU_layers = 3 
gru_units = 128
dense_units = 4 * gru_units
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""#### BiGRU3-256"""

number_of_hidden_GRU_layers = 3
gru_units = 256
dense_units = 4 * gru_units
train(number_of_hidden_GRU_layers, gru_units, dense_units)

"""## TextCNN"""

def ConvNet2(embedding_matrix,num_aux_targets):
  words = tf.keras.Input(shape=(None,))
  x = tf.keras.layers.Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)
  convs = []
  filter_sizes = [2,3,4,5,6]

  for filter_size in filter_sizes:
    l_conv = tf.keras.layers.Conv1D(filters=200, 
                        kernel_size=filter_size, 
                        activation='relu')(x)
    l_pool =tf.keras.layers.GlobalMaxPooling1D()(l_conv)
    convs.append(l_pool)
  
  l_merge = tf.concat(convs, axis=1)
  x = tf.keras.layers.Dropout(0.1)(l_merge)  
  x = tf.keras.layers.Dense(128, activation='relu')(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  result = tf.keras.layers.Dense(1, activation='sigmoid', name = 'target')(x)
  aux_result =  tf.keras.layers.Dense(num_aux_targets, activation='sigmoid', name = 'aux')(x)

  model = tf.keras.Model(inputs=words, outputs=[result, aux_result])
  model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

  return model

model = ConvNet2(embedding_matrix, y_train_aux.shape[-1])
model.fit(train_dataset, epochs=EPOCHS , callbacks=[callback])